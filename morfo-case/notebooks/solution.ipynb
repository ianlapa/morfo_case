{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa4f85ff",
   "metadata": {},
   "source": [
    "# Question 7 - OQ – How does the code handle corrupted image files?\n",
    "\n",
    "To ensure robustness, the analysis pipeline should handle corrupted or unreadable image files.\n",
    "\n",
    "For this, we can wrap the image loading logic in a `try/except` block inside the `analyze_batch()` function:\n",
    "\n",
    "```python\n",
    "try:\n",
    "    img = np.load(filepath)\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Skipping corrupted file {filename}: {e}\")\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9077c5a7",
   "metadata": {},
   "source": [
    "# Question 10 - OQ – AWS Design of Deployment\n",
    "\n",
    "To deploy this solution in a scalable and serverless way, I propose the following architecture on AWS:\n",
    "\n",
    "# AWS Components\n",
    "\n",
    "- **S3** to store `.npy` images (batches of 20)\n",
    "- **S3 Event Notifications + SQS** to track when new images are uploaded\n",
    "- **Lambda or Step Functions** to check if 100 images are present\n",
    "- **AWS Batch** to run the ETL logic in a container\n",
    "- **RDS (PostgreSQL)** to store summary statistics\n",
    "- **SNS** to notify users when processing is complete\n",
    "\n",
    "# Event Flow\n",
    "\n",
    "1. Images are uploaded to S3 (`raw/batch_x/img_y.npy`)\n",
    "2. S3 triggers events → SQS\n",
    "3. Lambda monitors the SQS or counts objects in S3\n",
    "4. When 100 images are present:\n",
    "   - Triggers an AWS Batch job (or Lambda Step Function)\n",
    "5. Job runs ETL logic (similar to current Docker setup)\n",
    "6. Results are stored in PostgreSQL\n",
    "7. Notification sent to user via SNS\n",
    "\n",
    "# Database Schema\n",
    "\n",
    "Table: `batch_results`\n",
    "\n",
    "| Field        | Type        \n",
    "\n",
    "| id           | UUID / PK   \n",
    "| batch_id     | TEXT        \n",
    "| white_avg    | FLOAT       \n",
    "| white_std    | FLOAT       \n",
    "| white_min    | INT         \n",
    "| white_max    | INT         \n",
    "| black_avg    | FLOAT       \n",
    "| black_std    | FLOAT       \n",
    "| black_min    | INT         \n",
    "| black_max    | INT         \n",
    "| processed_at | TIMESTAMP   "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
